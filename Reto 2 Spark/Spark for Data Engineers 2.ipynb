{"cells":[{"cell_type":"markdown","source":["# **Reto II**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6931f361-3d08-4e08-adae-af76ed5a4ecf"}}},{"cell_type":"markdown","source":["### 1. Dataset\n\nLos datos de origen son proporcionados en un archivos csv:\n\n* udfs: dataset con datos de operaciones financieras.\n\n### 2. Columnas y significado:\n\n* nb: número de referencia de la operación.\n* contract: identificador de contrato.\n* udf_ref: identificador de operación de trading.\n* fmly: familia a la que pertenece la operación financiera.\n* grp: grupo al que pertenece la operación financiera.\n* type: tipo de operación financiera.\n* country: país de origen de la operación.\n* udf_name: campo informado en el registro.\n* num_value: valor numérico.\n* string_value: valor de cadena de caracteres.\n* date_value: valor de fecha.\n* data_timestamp_part: marca temporal.\n* data_date_part: fecha en la que se almacena la información.\n* source_system: fuente de los datos.\n\n### 3. Descripción del problema:\n\nSi hacemos una visión general a nuestro conjunto de datos, podemos observar como hay hasta 10 registros (filas) para cada valor de *nb*, donde cada registro solo da información para un valor de *udf_name*. Esto es un gasto innecesario de almacenamiento y computación, además de complicar los futuros cálculos derivados de estos datos. Por esta razón, necesitamos convertir estos registros con el mismo *nb* a un solo registro.\n\nNuestro dataframe final tendrá que contener las siguientes columnas: `nb, M_CCY, M_CLIENT, M_CRDTCHRG, M_DIRECTIAV, M_DISCMARGIN, M_LIQDTYCHRG, M_MVA, M_RVA, M_SELLER, M_SUCURSAL`\n\n* nb: debe contener el número de referencia de la operación.\n* M_CLIENT, M_SELLER, M_CCY, M_SUCURSAL: deben mapear el valor de *string_value*\n* M_DISCMARGIN, M_DIRECTIAV, M_LIQDTYCHRG, M_CRDTCHRG, , M_MVA, M_RVA: deben mapear el valor de *num_value*\n\n\nUna vez tengamos este resultado, necesitaremos eliminar las operaciones que no tengan informados ninguno de los siguientes campos:\n\nM_DISCMARGIN, M_DIRECTIAV, M_LIQDTYCHRG, M_CRDTCHRG, M_MVA, M_RVA, M_SELLER\n\nNo informados en este caso significa que o son valores nulos, vacíos o 0, en el caso de los campos numéricos.\n\n### 4. Reto:\n\n* Obtener un dataframe final que contenga las columnas indicadas, con un registro por *nb* y con los valores correctos mapeados.\n* Las operaciones con los campos M_DISCMARGIN, M_DIRECTIAV, M_LIQDTYCHRG, M_CRDTCHRG, , M_MVA, M_RVA, M_SELLER no informados no deben existir.\n* Hacerlo de la manera más eficiente posible a nivel computacional.\n\n**NOTA:** Cada uno de los pasos descritos en el problema pueden efectuarse en una sola línea."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b2c604f-94dc-47ca-accf-8e431c4e77cd"}}},{"cell_type":"code","source":["#Se carga el archivo\nudfs = spark.read\\\n            .format(\"csv\")\\\n            .option(\"header\", \"true\")\\\n            .option(\"delimiter\", \";\")\\\n            .load(\"/FileStore/tables/udfs.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d2c18e1-f6a0-45ae-a5a9-953645560278"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#se hace un pivot para las columnas string\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import first\nudfs_pivot_string = udfs.groupBy(\"nb\")\\\n                        .pivot(\"udf_name\")\\\n                        .agg(first(\"string_value\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf4dd797-8995-4a28-a6e8-737b47ae5ba0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#pivot para columnas numericas\nudfs_pivot_num = udfs.groupBy(\"nb\")\\\n                 .pivot(\"udf_name\")\\\n                 .agg(F.sum(\"num_value\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77e039c0-1b6b-411a-b9cb-d2e944a3ad35"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#hacemos un join y seleccionamos las columnas que nos importan\njoinedDF = udfs_pivot_string.join(udfs_pivot_num, udfs_pivot_string.nb == udfs_pivot_num.nb, \"left\")\\\n           .select(udfs_pivot_string.nb,udfs_pivot_string.M_CCY, udfs_pivot_string.M_CLIENT, udfs_pivot_num.M_CRDTCHRG,       udfs_pivot_num.M_DIRECTIAV,udfs_pivot_num.M_DISCMARGIN, udfs_pivot_num.M_LIQDTYCHRG, udfs_pivot_num.M_MVA, udfs_pivot_num.M_RVA, udfs_pivot_string.M_SELLER, udfs_pivot_string.M_SUCURSAL)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"173a0e8b-5fde-4d8d-bd3f-1fc9000e1f28"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#quitamos las líneas que cumplen con la condición\nresultado = joinedDF.filter(((F.col(\"M_DISCMARGIN\") != 0.0))\\\n| ((F.col(\"M_DIRECTIAV\") != 0.0))\\\n| ((F.col(\"M_LIQDTYCHRG\") != 0.0))\\\n| ((F.col(\"M_CRDTCHRG\") != 0.0))\\\n| ((F.col(\"M_MVA\") != 0.0)) | ((F.col(\"M_RVA\") != 0.0))\\\n| (F.col(\"M_SELLER\").isNotNull()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f98c2bb-1b8f-4122-882b-b308758206c6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Resultado:\n\n**INSTRUCCIONES**: El DataFrame resultante debe almacenarse en la variable `resultado`, sustituyendo el valor `None` por el código que consideréis oportuno. De esta forma podréis comprobar si el resultado es correcto."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08b6912f-5ea6-4a6f-80e3-1942f682add7"}}},{"cell_type":"code","source":["\nassert(resultado.count() == 60)\nassert(len(resultado.columns) == 11)\nassert(resultado.columns[4] == \"M_DIRECTIAV\")\nassert(resultado.select(\"M_SELLER\").filter(F.col(\"nb\") == 23037162).collect()[0].__getitem__('M_SELLER') == 'AMAM') \nassert(resultado.select(\"M_SELLER\").filter(F.col(\"nb\") == 19665186).collect()[0].__getitem__('M_SELLER') == \"LB_VSTAVRE\")\nassert(resultado.select(\"M_RVA\").filter(F.col(\"nb\") == 444111222).collect()[0].__getitem__('M_RVA') == 8956)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"186c45c5-3e5b-453a-b728-de23574ada8e"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"spylon-kernel","language":"scala","name":"spylon-kernel"},"language_info":{"mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","codemirror_mode":"text/x-scala","help_links":[{"text":"MetaKernel Magics","url":"https://metakernel.readthedocs.io/en/latest/source/README.html"}],"version":"0.4.1","file_extension":".scala"},"application/vnd.databricks.v1+notebook":{"notebookName":"Spark for Data Engineers 2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":267011510791591}},"nbformat":4,"nbformat_minor":0}
